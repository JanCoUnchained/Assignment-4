---
title: "A4_jan"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pacman)
p_load(tidyverse, readxl, brms, metafor, brmstools, ggunchained)

pitch <- read_excel("Assignment4PitchDatav2.xlsx")
meta <- read_excel("Assignment4MetaData.xlsx")
```

# TASK 1

```{r meta situation}
meta %>%
  filter(!is.na(MeanES)) %>% #no NA. 
  ggplot(aes(x = MeanES, y = StudyRef)) + 
  geom_segment(aes(x = MeanES-SdES, xend = MeanES+SdES, y=StudyRef, yend=StudyRef)) +
  geom_point() +
  geom_vline(xintercept = 0, color = "grey42") +
  theme_janco_point()
```

```{r standard error and model}

#not necessary?
meta2 <- meta %>%
  # counting sei = standard deviation / sqrt()
  mutate(sample = SAMPLE_SIZE_SZ + SAMPLE_SIZE_CT,
         sei = SdES / sqrt(sample)) %>%
  # filter non complete cases
  filter(!is.na(MeanES))


# Matti model 
brm_matti <- brm(
  MeanES | se(VarianceES) ~ 1 + (1 | StudyRef), #works, we know the variance. 
  prior = set_prior("uniform(0, 1000)", class = "sd"), 
  data = meta2, 
  iter = 10000,
  cores = 4
) 

#effect size?
plot(brm_matti)
pairs(brm_matti)
brm_matti #est error = 0.25 (much bigger than the other models).

```

nice plot. 

```{r meta brm forest}
brmstools::forest(brm_matti,
       show_data = TRUE,
       av_name = "Effect size")
```


# TASK 2
#standardizing. 

```{r summarize}
#overview (I don't think we actually use this)
pitch_sum <- pitch %>%
  group_by(studynr, diagnosis) %>%
  summarise(mean = mean(PitchSD), n = n())

#scaling & centering [z-scale] 
pitch2 <- pitch %>%
  mutate(PitchSD_s = as.vector(scale(PitchSD)))

```


# TASK 3
```{r t3 priors}

```


```{r t3 model bogus prior}

#lognormal transformation & then z-scaling
pitch2$diagnosis <- as.factor(pitch2$diagnosis)

pitch3 <- pitch2 %>%
  mutate(pitchSDLOG = log(PitchSD),
         pitchSDLOGz = scale(pitchSDLOG))


#change has been made here: (1|trial) added
#which random effects?
mod_f <- bf("pitchSDLOG ~ diagnosis + (1|studynr) + (1 | ID) + (1|trial)")

brm_mod <- brm(
  mod_f, 
  prior = c(
    prior(normal(2.5, 0.5), class = "Intercept"),
    prior(normal(0, 0.5), class = "sd"),
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 1), class = "sigma")),
  data = pitch3, 
  cores = 4, iter = 4000, warmup = 2000
)

#pp_check looks weird
pp_check(brm_mod, nsamples = 50) #what does it do?
brms::marginal_effects(brm_mod) #marginal effect predicted. 

#the model 
brm_mod
plot(brm_mod) #looks good. 
pairs(brm_mod) #looks good --> answering a question as well. 

```

simulating from the model

```{r}


#posterior samples
library(rethinking)
samples <- posterior_samples(brm_mod, pars = "b") #all effects.  
head(samples)
dens(samples)
samples1 <- posterior_samples(brm_mod)
fitted1 <- fitted(brm_mod)

#plot predictions. 
mod_line <- fitted(brm_mod) %>% #population parameters (fitted).
  cbind.data.frame(pitch2)

#predictions plot split violin
p1 <- mod_line %>%
  ggplot(aes(x = diagnosis, y = Estimate,
             color = factor(diagnosis),
             fill = factor(diagnosis))) +
  geom_split_violin() +
  geom_jitter(alpha = 0.4, width = 0.4)

#plot split violin 
p2 <- pitch2 %>%
  ggplot(aes(x = diagnosis, y = PitchSD_s,
             color = factor(diagnosis),
             fill = factor(diagnosis)))+
  geom_split_violin() +
  geom_jitter(alpha = 0.4, width = 0.4) +
  ylim(-1, 2)

#studies plot data
p3 <- pitch2 %>%
  ggplot(aes(x = diagnosis, y = PitchSD_s,
             color = factor(diagnosis),
             fill = factor(diagnosis)))+
  geom_split_violin() +
  geom_jitter(alpha = 0.4, width = 0.4) +
  facet_wrap(~studynr)

#studies plot prediction
p4 <- mod_line %>%
  ggplot(aes(x = diagnosis, y = Estimate,
             color = factor(diagnosis),
             fill = factor(diagnosis)))+
  geom_split_violin() +
  geom_jitter(alpha = 0.4, width = 0.4) +
  facet_wrap(~studynr)

#summary
library(cowplot)
plot_grid(p1, p2)
plot_grid(p3, p4)


```


# TASK 4
```{r}

#updating prior on the group difference. 
#not a big difference. 
meta_mod <- brm(
  mod_f, 
  prior = c(
    prior(normal(0, 0.5), class = "Intercept"),
    #prior(normal(0, 2), class = "sd", group = "studynr"),
    prior(normal(0, 0.5), class = "sd"),
    prior(normal(-0.57, 0.25), class = "b"), #has very little weight. 
    prior(normal(0, 1), class = "sigma")),
  data = pitch2, 
  cores = 4, iter = 4000, warmup = 2000
)

```

checking for all the models 

```{r}
#posterior checks
pp_check(brm_mod) #fuck
pp_check(meta_mod) #fuck
pp_check(brm_matti) #good

#no difference between bogus priors & meta priors
brm_mod
meta_mod
brm_matti

#systematic error in divergent transitions?
pairs(brm_mod, np = nuts_params(brm_mod)) #

```

Q6 comparison.
Almost the same. 

```{r}
# criterion. 
library(pacman)
p_load(tidyverse, rethinking, brms, brmstools)
b1 <- add_criterion(meta_mod, "waic")
b2 <- add_criterion(brm_mod, "waic")
w <- loo_compare(b1, b2, criterion = "waic")
print(w, simplify = F)

#comparing them 
waic(meta_mod, brm_mod)

```

